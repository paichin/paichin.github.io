---
title: 深度学习调参技巧
layout: post
categories: 'Machine Learning'
tags: ''
---

作为面试胡言乱语爱好者，这篇从回答面试问题的角度来扯调参那些事。调参包括以下几个方面：

1.参数初始化<br>
2.模型结构<br>
3.损失函数<br>
4.优化方法<br>
5.训练过程<br>

下面分别说一下：

1.参数初始化

一般使用正态分布初始化，但是在较深的网络中，因为方差在每一层不断叠加，初始化参数过大或过小都可能引发输出的爆炸或者消失。<br>
所以，要根据神经网络的特点选择不同的初始化策略，如基于扇入扇出，对[-1,1]均匀分布进行了缩放的Xavier初始化是针对普通激活函数如tanh,sigmoid的初始化方法，而根据正态分布进行缩放的Kaiming初始化方法适合Relu激活函数。如果感觉输出效果提不上来的话可以试试切换各种初始化方法。

2.模型结构

dropout，相当于构建了2^n个神经网络，起到类似bagging的作用<br>
隐藏层单元数的调整：模型越复杂隐藏层单元数越大，可增加隐藏层单元数直到validation_error变高。<br>
batch_normalization，如果某层之后的数据分布都在激活函数很低的区域，可能造成梯度消失。用batch_normalization调整到(0,1)的正态分布。但是这样的正态分布未必可以很好地描述数据的真实分布，所以要加上可学习的偏置项。

3.损失函数

一般分类就是categorical-cross-entropy+softmax，回归就是L2的loss。但是loss也要规范化，尤其回归问题的loss可能会很大。<br>
多任务，loss限制在一个量级上。<br>

4.优化方法

学习率：如果用了自适应方法，可以选择不用调整学习率。否则可以从0.01开始进行调整到0.001等。

warm up：在训练中先用一个很小的学习率，再逐渐增大。因为模型权重是随机初始化的，一开始就用较大学习率可能造成模型的震荡。用warm up的方式可以让开始时训练的几个epoch或者steps内学习率较小，模型渐渐趋于稳定，再使用预先设置的学习率使得收敛更快。

5.训练过程

随着一个一个epoch跑过，模型一般会从欠拟合到过拟合。

用early stopping可以确定epoch的数量。

根据train_loss和val_loss的上升下降情况观察参数应该如何调整。<br>
如都下降，说明网络仍在学习；如train_loss下降，val_loss上升，说明过拟合。如train_loss下降，val_loss不变，欠拟合。train_loss,val_loss均不变，陷入瓶颈。train_loss不断上升，val_loss不断上升，模型有问题。train_loss上升，val_loss下降，数据集有问题。……<br>

<br>
感觉面试说说这些也差不多了，先这样。